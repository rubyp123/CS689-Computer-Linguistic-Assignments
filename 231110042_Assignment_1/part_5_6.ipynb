{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a9dd60",
   "metadata": {},
   "source": [
    "# Part 5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5559be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(sentence):\n",
    "    hindi_punctuation = ['।', '॥', ',', '?', '\"', \"'\", ';', '-', '(', ')', '[', ']', '{', '}', '...','!','.']\n",
    "    for punctuation in hindi_punctuation:\n",
    "        sentence = sentence.replace(punctuation, '')\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60c79235",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cs689_assignment.txt', 'r', encoding='utf-8') as file:\n",
    "    Inp = file.readlines()\n",
    "\n",
    "# for i in Inp:\n",
    "#     print(i)\n",
    "\n",
    "corpus=[]\n",
    "ground_truth = []\n",
    "\n",
    "i=0\n",
    "while (i<(len(Inp)-1)):\n",
    "    x = Inp[i]\n",
    "    x=x[2:-1]\n",
    "    corpus.append(x)\n",
    "    ground_truth.append(Inp[i+1])\n",
    "    i+=3\n",
    "\n",
    "token = []\n",
    "for i in ground_truth:\n",
    "    l = i.split(',')\n",
    "    x = l[len(l)-1]\n",
    "    x=x[:-1]\n",
    "    l=l[:-1]\n",
    "    l.append(x)\n",
    "    token.append(l)\n",
    "    \n",
    "# print(len(corpus) , len(token))\n",
    "# print(corpus)\n",
    "# print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28c7509d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_underscore( tokens):\n",
    "    token=[]\n",
    "    for i in tokens:\n",
    "        j = i.lstrip('▁') \n",
    "        k = j.lstrip('#')\n",
    "        token.append(k)\n",
    "    return token\n",
    "    \n",
    "\n",
    "\n",
    "def find_True_positive(my_token ,  model_token):\n",
    "    res = 0\n",
    "    for i in my_token:\n",
    "        if i in model_token:\n",
    "            res+=1\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "def find_false_negative(my_token , model_token):\n",
    "    res = 0\n",
    "    for i in model_token:\n",
    "        if i not in my_token:\n",
    "            res+=1\n",
    "    return res\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_false_positive(my_token , model_token):\n",
    "    res = 0\n",
    "    for i in my_token:\n",
    "        if i not in model_token:\n",
    "            res+=1\n",
    "    return res\n",
    "    \n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6bf131",
   "metadata": {},
   "source": [
    "# Unigram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc2707d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=hi_100.txt --model_prefix=m_unigram --vocab_size=2000 --model_type=unigram') \n",
    "sp_unigram = spm.SentencePieceProcessor() \n",
    "sp_unigram.load('m_unigram.model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99768e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.04067796610169491\n",
      "Recall :  0.012461059190031152\n",
      "F1-Score :  0.019077901430842606\n"
     ]
    }
   ],
   "source": [
    "tp=0\n",
    "fn=0\n",
    "fp=0\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    Input = remove_punctuation(corpus[i])\n",
    "    m_token = sp_unigram.encode_as_pieces(Input)\n",
    "    model_token = remove_underscore(m_token) \n",
    "    my_token = token[i]\n",
    "#     print(\"model : \",model_token)\n",
    "#     print(\"my_token : \",my_token)\n",
    "    tp += find_True_positive(my_token , model_token)\n",
    "    fn += find_false_negative(my_token , model_token)\n",
    "    fp += find_false_positive(my_token , model_token)\n",
    "    \n",
    "    \n",
    "precision = (tp)/(tp + fp)\n",
    "recall = (tp)/(tp + fn)\n",
    "f_score =  (2*precision * recall ) / (precision + recall)\n",
    "\n",
    "print(\"Precision : \",precision)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"F1-Score : \",f_score)\n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30ddfe0",
   "metadata": {},
   "source": [
    "# BPE Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345040f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "spm.SentencePieceTrainer.train('--input=hi_100.txt --model_prefix=m_bpe --vocab_size=2000 --model_type=bpe') \n",
    "sp_bpe = spm.SentencePieceProcessor() \n",
    "sp_bpe.load('m_bpe.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7466a4c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4a4ee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.03728813559322034\n",
      "Recall :  0.011\n",
      "F1-Score :  0.016988416988416986\n"
     ]
    }
   ],
   "source": [
    "tp=0\n",
    "fn=0\n",
    "fp=0\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    Input = remove_punctuation(corpus[i])\n",
    "    m_tokens = sp_bpe.encode_as_pieces(Input)\n",
    "    model_token = remove_underscore(m_tokens) \n",
    "    my_token = token[i]\n",
    "#     print(\"model : \",model_token)\n",
    "#     print(\"my_token : \",my_token)\n",
    "    tp += find_True_positive(my_token , model_token)\n",
    "    fn += find_false_negative(my_token , model_token)\n",
    "    fp += find_false_positive(my_token , model_token)\n",
    "    \n",
    "precision = (tp)/(tp + fp)\n",
    "recall = (tp)/(tp + fn)\n",
    "\n",
    "if((precision + recall)==0):\n",
    "    f_score=0\n",
    "else:\n",
    "    f_score =  (2*precision * recall ) / (precision + recall)\n",
    "\n",
    "print(\"Precision : \",precision)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"F1-Score : \",f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69566b1",
   "metadata": {},
   "source": [
    "# M-BERT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6197f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False , max_len=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d47cf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.03728813559322034\n",
      "Recall :  0.009482758620689655\n",
      "F1-Score :  0.015120274914089349\n"
     ]
    }
   ],
   "source": [
    "tp=0\n",
    "fn=0\n",
    "fp=0\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    Input = remove_punctuation(corpus[i])\n",
    "    m_tokens = tokenizer.tokenize(Input)\n",
    "    model_token = remove_underscore(m_tokens) \n",
    "    my_token = token[i]\n",
    "#     print(\"model : \",model_token)\n",
    "#     print(\"my_token : \",my_token)\n",
    "    tp += find_True_positive(my_token , model_token)\n",
    "    fn += find_false_negative(my_token , model_token)\n",
    "    fp += find_false_positive(my_token , model_token)\n",
    "    \n",
    "precision = (tp)/(tp + fp)\n",
    "recall = (tp)/(tp + fn)\n",
    "\n",
    "if((precision + recall)==0):\n",
    "    f_score=0\n",
    "else:\n",
    "    f_score =  (2*precision * recall ) / (precision + recall)\n",
    "\n",
    "print(\"Precision : \",precision)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"F1-Score : \",f_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e25bc6",
   "metadata": {},
   "source": [
    "# Indic-BERT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ae33962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('ai4bharat/indic-bert', use_fast=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb719636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.05084745762711865\n",
      "Recall :  0.018656716417910446\n",
      "F1-Score :  0.0272975432211101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tp=0\n",
    "fn=0\n",
    "fp=0\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    Input = remove_punctuation(corpus[i])\n",
    "    t = tokenizer(Input , return_tensors=\"pt\")\n",
    "    m_tokens = tokenizer.convert_ids_to_tokens(t['input_ids'][0])\n",
    "    model_token = remove_underscore(m_tokens) \n",
    "    my_token = token[i]\n",
    "#     print(\"model : \",model_token)\n",
    "#     print(\"my_token : \",my_token)\n",
    "    tp += find_True_positive(my_token , model_token)\n",
    "    fn += find_false_negative(my_token , model_token)\n",
    "    fp += find_false_positive(my_token , model_token)\n",
    "    \n",
    "precision = (tp)/(tp + fp)\n",
    "recall = (tp)/(tp + fn)\n",
    "\n",
    "if((precision + recall)==0):\n",
    "    f_score=0\n",
    "else:\n",
    "    f_score =  (2*precision * recall ) / (precision + recall)\n",
    "\n",
    "print(\"Precision : \",precision)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"F1-Score : \",f_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa671a3",
   "metadata": {},
   "source": [
    "# White-Space Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1e62a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision :  0.061016949152542375\n",
      "Recall :  0.0313588850174216\n",
      "F1-Score :  0.041426927502876874\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tp=0\n",
    "fn=0\n",
    "fp=0\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    Input = remove_punctuation(corpus[i])\n",
    "    m_tokens = Input.split()\n",
    "    model_token = remove_underscore(m_tokens) \n",
    "    my_token = token[i]\n",
    "#     print(\"model : \",model_token)\n",
    "#     print(\"my_token : \",my_token)\n",
    "    tp += find_True_positive(my_token , model_token)\n",
    "    fn += find_false_negative(my_token , model_token)\n",
    "    fp += find_false_positive(my_token , model_token)\n",
    "    \n",
    "precision = (tp)/(tp + fp)\n",
    "recall = (tp)/(tp + fn)\n",
    "\n",
    "if((precision + recall)==0):\n",
    "    f_score=0\n",
    "else:\n",
    "    f_score =  (2*precision * recall ) / (precision + recall)\n",
    "\n",
    "print(\"Precision : \",precision)\n",
    "print(\"Recall : \",recall)\n",
    "print(\"F1-Score : \",f_score)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed6ae6",
   "metadata": {},
   "source": [
    "# Part - 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c73bf2a",
   "metadata": {},
   "source": [
    "## Model Evaluation Results\n",
    "\n",
    "### Unigram Model:\n",
    "- **Precision** :  0.04067796610169491\n",
    "- **Recall** :  0.012461059190031152\n",
    "- **F1-Score** :  0.019077901430842606\n",
    "- This model is not very precise and misses a lot of relevant information. It doesn't do well at capturing the right words.\n",
    "\n",
    "### BPE Model:\n",
    "- **Precision** :  0.03728813559322034\n",
    "- **Recall** :  0.011\n",
    "- **F1-Score** :  0.016988416988416986\n",
    "- Similar to the Unigram Model, this one also lacks precision and struggles to find important. This model also not very precise ans also misses relevent information.\n",
    "\n",
    "### M-BERT Model:\n",
    "- **Precision** :  0.05084745762711865\n",
    "- **Recall** :  0.01989389920424403\n",
    "- **F1-Score** :  0.02859866539561487\n",
    "- This model performs better than the Unigram and BPE models. It's more accurate at finding the right words but still has room for improvement.\n",
    "\n",
    "### Indic-BERT Model:\n",
    "- **Precision** :  0.05084745762711865\n",
    "- **Recall** :  0.018656716417910446\n",
    "- **F1-Score** :  0.0272975432211101\n",
    "- Similar to M-BERT, this model is good at finding relevant information but has some limitations.\n",
    "\n",
    "### White-Space Tokenizer:\n",
    "- **Precision** :  0.061016949152542375\n",
    "- **Recall** :  0.0313588850174216\n",
    "- **F1-Score** :  0.041426927502876874\n",
    "- This model performs the best among all.The White-Space Tokenizer gives the highest precision, recall, and F-score among all tokenizers.It's more accurate and effective at capturing relevant information compared to the others.\n",
    "\n",
    "### Summary : \n",
    "- while M-BERT and Indic-BERT Models are better than Unigram and BPE Models, the White-Space Tokenizer stands out as the most effective in terms of precision, recall, and F1-score. It does a better job of capturing the right words and is the most reliable option for tokenization. \n",
    "\n",
    " - The problem with Hindi language arises because the computer sometimes doesn't understand how to correctly count and process the characters. This makes it hard for the models to break down the text into meaningful parts, which affects their ability to understand Hindi properly. So, fixing this issue is crucial for making sure the models can accurately handle Hindi text.\n",
    " \n",
    " - In the dataset used for training the model, there are specific words in Hindi known as Karaka words. the model may tokenize these Karaka words separately, that makes difference between the model's output and the ground truth. \n",
    " \n",
    " - In Hindi, there are words that are made by putting two or more words together,  which is called \"sandhi\" and \"Samasha\". For models, figuring out where one word starts and another ends in these situations can be hard. ans also how to tokenize these kind of word can be hard. This makes it tricky for them to understand and work with Hindi text.\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f438d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
