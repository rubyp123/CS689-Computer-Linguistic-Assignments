{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b4be63-58dc-48fe-ab9a-69d9969272a5",
   "metadata": {},
   "source": [
    "# ChatGPT vs Ground Truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909fa4b2-cb0e-48f0-a973-863f489b0a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-LOC       0.20      0.17      0.18         6\n",
      "      B-MISC       0.00      0.00      0.00        31\n",
      "       B-ORG       0.20      0.50      0.29         2\n",
      "       B-PER       0.50      0.50      0.50         6\n",
      "       I-LOC       0.00      0.00      0.00         2\n",
      "      I-MISC       0.00      0.00      0.00        18\n",
      "       I-ORG       1.00      0.17      0.29         6\n",
      "       I-PER       0.67      0.67      0.67         6\n",
      "           O       0.91      0.99      0.95       519\n",
      "\n",
      "    accuracy                           0.88       596\n",
      "   macro avg       0.39      0.33      0.32       596\n",
      "weighted avg       0.82      0.88      0.84       596\n",
      "\n",
      "596\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "with open('cs689_assignment.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "    Inp1 = file.readlines()\n",
    "\n",
    "with open('chatgpt_answers.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "    Inp2 = file.readlines()\n",
    "\n",
    "corpus=[]\n",
    "ground_truth = []\n",
    "chatgpt = []\n",
    "\n",
    "i=0\n",
    "while (i<(len(Inp1)-1)):\n",
    "    x = Inp1[i]\n",
    "    x=x[2:-1]\n",
    "    corpus.append(x)\n",
    "    ground_truth.append(Inp1[i+1])\n",
    "    i+=3\n",
    "\n",
    "i=0\n",
    "while (i<(len(Inp2)-1)):\n",
    "    x = Inp2[i]\n",
    "    x=x[2:-1]\n",
    "    chatgpt.append(Inp2[i+1])\n",
    "    i+=3\n",
    "\n",
    "g = []\n",
    "c = []\n",
    "labels_list=[]\n",
    "\n",
    "for i in range(25):\n",
    "    labels_list = ground_truth[i].strip().split()\n",
    "    g.append(labels_list)\n",
    "    labels_list = chatgpt[i].strip().split()\n",
    "    c.append(labels_list)\n",
    "\n",
    "\n",
    "# print(g)\n",
    "# print(c)\n",
    "\n",
    "ground_truth_flat = [label for sublist in g for label in sublist]\n",
    "predicted_labels_flat = [label for sublist in c for label in sublist]\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(ground_truth_flat, predicted_labels_flat))\n",
    "\n",
    "print(len(ground_truth_flat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "290594b6-1fdf-4590-aa7f-3bc0851e2dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.31847877596491275\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_scores = f1_score(ground_truth_flat, predicted_labels_flat, labels=['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'], average=None)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1 = sum(f1_scores) / len(f1_scores)\n",
    "\n",
    "print(\"Macro F1 Score:\", macro_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5c04fe-d508-42ca-be8d-11744e5c3f5e",
   "metadata": {},
   "source": [
    "# Ground Truth vs indicBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9246bc8-a45d-4c98-a837-58827f64976d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "with open('tags_output_BERT.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "     bert_tags = file.readlines()\n",
    "\n",
    "# print(bert_tags)\n",
    "# print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "878db4cb-6044-4623-906e-34051a326ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "for i in bert_tags:\n",
    "    temp = i.strip().split()\n",
    "    b.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bf0ddcb6-7028-4dfe-82d8-369a9aa9c36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(b)\n",
    "# print(g)\n",
    "\n",
    "for i in range(25):\n",
    "    x = len(b[i])\n",
    "    y = len(g[i])\n",
    "    while(x<y):\n",
    "        b[i].append('NA')\n",
    "        x = len(b[i])\n",
    "        y = len(g[i])\n",
    "        \n",
    "    while(x>y):\n",
    "        b[i].pop()\n",
    "        x = len(b[i])\n",
    "        y = len(g[i])\n",
    "        \n",
    "        \n",
    "        \n",
    "# for i in range(25):\n",
    "#     print(i,len(b[i]),len(g[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "103b814e-c6fd-481b-acb1-27cf26cc7957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      I-MISC       0.00      0.00      0.00        18\n",
      "       I-ORG       0.00      0.00      0.00         6\n",
      "       I-PER       0.20      0.17      0.18         6\n",
      "      B-MISC       0.00      0.00      0.00        31\n",
      "       I-LOC       0.00      0.00      0.00         2\n",
      "           O       0.89      0.91      0.90       519\n",
      "       B-PER       0.14      0.17      0.15         6\n",
      "       B-ORG       0.00      0.00      0.00         2\n",
      "       B-LOC       0.00      0.00      0.00         6\n",
      "\n",
      "   micro avg       0.84      0.80      0.82       596\n",
      "   macro avg       0.14      0.14      0.14       596\n",
      "weighted avg       0.78      0.80      0.79       596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rubyp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rubyp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rubyp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ground_truth_flat_bert = [label for sublist in g for label in sublist]\n",
    "predicted_labels_flat_bert = [label for sublist in b for label in sublist]\n",
    "\n",
    "# Print classification report\n",
    "# print(classification_report(ground_truth_flat_bert, predicted_labels_flat_bert))\n",
    "\n",
    "print(classification_report(ground_truth_flat_bert, predicted_labels_flat_bert, labels=[label for label in set(ground_truth_flat_bert) if label != 'NA']))\n",
    "\n",
    "# print(len(ground_truth_flat_bert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e3819f56-ea86-408c-91c5-249ccc186752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.1370951703967879\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_scores_bert = f1_score(ground_truth_flat_bert, predicted_labels_flat_bert, labels=['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'], average=None)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1_bert = sum(f1_scores_bert) / len(f1_scores_bert)\n",
    "\n",
    "print(\"Macro F1 Score:\", macro_f1_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44e12c2-d326-4f8a-ad79-8b8244ec11f9",
   "metadata": {},
   "source": [
    "# Ground Truth vs indicNER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "471aa758-7b8c-4e68-aae0-8657b9cc8eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "with open('tags_output_NER.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "     ner_tags = file.readlines()\n",
    "\n",
    "# print(ner_tags)\n",
    "# print(g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "264e54f8-1585-481b-ac68-d216066c8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=[]\n",
    "for i in ner_tags:\n",
    "    temp = i.strip().split()\n",
    "    n.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c84d2b3-01de-498c-a7d9-be24ddbf0361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(n)\n",
    "# print(g)\n",
    "\n",
    "for i in range(25):\n",
    "    x = len(n[i])\n",
    "    y = len(g[i])\n",
    "    while(x<y):\n",
    "        n[i].append('NA')\n",
    "        x = len(n[i])\n",
    "        y = len(g[i])\n",
    "        \n",
    "    while(x>y):\n",
    "        n[i].pop()\n",
    "        x = len(n[i])\n",
    "        y = len(g[i])\n",
    "        \n",
    "        \n",
    "        \n",
    "# for i in range(25):\n",
    "#     print(i,len(b[i]),len(g[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "6e7eb090-d08a-4352-9ee9-eb6a76b73226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      I-MISC       0.00      0.00      0.00        18\n",
      "       I-ORG       0.50      0.33      0.40         6\n",
      "       I-PER       0.20      0.17      0.18         6\n",
      "      B-MISC       0.00      0.00      0.00        31\n",
      "       I-LOC       0.00      0.00      0.00         2\n",
      "           O       0.91      0.93      0.92       519\n",
      "       B-PER       0.22      0.33      0.27         6\n",
      "       B-ORG       0.12      0.50      0.20         2\n",
      "       B-LOC       0.25      0.50      0.33         6\n",
      "\n",
      "   micro avg       0.86      0.82      0.84       596\n",
      "   macro avg       0.25      0.31      0.26       596\n",
      "weighted avg       0.80      0.82      0.81       596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rubyp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rubyp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\rubyp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "ground_truth_flat_ner = [label for sublist in g for label in sublist ]\n",
    "predicted_labels_flat_ner = [label for sublist in n for label in sublist]\n",
    "\n",
    "# Print classification report\n",
    "# print(classification_report(ground_truth_flat_ner, predicted_labels_flat_ner))\n",
    "\n",
    "print(classification_report(ground_truth_flat_ner, predicted_labels_flat_ner, labels=[label for label in set(ground_truth_flat_ner) if label != 'NA']))\n",
    "\n",
    "# print(len(ground_truth_flat_ner))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "b5142827-82c9-42fc-b9b1-1b7767aebb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.255528568123988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_scores_ner = f1_score(ground_truth_flat_ner, predicted_labels_flat_ner, labels=['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'], average=None)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1_ner = sum(f1_scores_ner) / len(f1_scores_ner)\n",
    "\n",
    "print(\"Macro F1 Score:\", macro_f1_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "a182f730-4add-407f-87bc-4067af6fb907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(25):\n",
    "#     print(b[i],n[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63172510-a808-478d-bd58-7eee4d501472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
