{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267bc63f-3991-4d2a-a7c5-6e3abd6d52f0",
   "metadata": {},
   "source": [
    "# Indic-BERT Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f76defe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback, IntervalStrategy\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer_indicBERT/kaggle/working/tokenizer_indicBERT\")\n",
    "model_fine_tuned = AutoModelForTokenClassification.from_pretrained(\"my_indicBERT/kaggle/working/my_indicBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "662fe8f8-9475-493e-82a6-fd808118d380",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's download the Naampadam (Indic NER) dataset\n",
    "\n",
    "from datasets import ClassLabel, load_dataset, load_metric, DownloadMode\n",
    "\n",
    "lang='hi'\n",
    "\n",
    "raw_datasets = load_dataset('ai4bharat/naamapadam', lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98946b86-b5eb-4077-b1ef-fac7cd9d2d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(raw_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3683ea7b-bd71-4aa6-8a5c-78871ec63d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all texts and align the labels with them.\n",
    "padding = \"max_length\"\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[text_column_name],\n",
    "        padding=padding,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n",
    "        is_split_into_words=True,\n",
    "    )\n",
    "    # print(tokenized_inputs)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[label_column_name]):\n",
    "        # print('=====')\n",
    "        # print('{} {}'.format(i,label)) #ak\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    # print(tokenized_inputs)\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ebd5753-c9bb-4ad3-a7a0-6873d5f0685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_test  = raw_datasets[\"test\"]\n",
    "# print(type(train_dataset))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "803a9259-f671-4e48-8cba-84ca7d951f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(train_dataset_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "74b33ee8-1fbe-4136-b274-060758c7529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    0: 'O',\n",
    "    1: 'B-PER',\n",
    "    2: 'I-PER',\n",
    "    3: 'B-ORG',\n",
    "    4: 'I-ORG',\n",
    "    5: 'B-LOC',\n",
    "    6: 'I-LOC'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "992d0249-cd62-4d18-b49b-070ed29a47f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_dataset_test\n",
    "# print(sample)\n",
    "\n",
    "test_data=[]\n",
    "\n",
    "for i in sample['tokens']:\n",
    "    sentence = ' '.join(i)\n",
    "    test_data.append(sentence)\n",
    "\n",
    "ground_truth = []\n",
    "\n",
    "for i in sample['ner_tags']:\n",
    "    output_list = [label_mapping[num] for num in i]\n",
    "    ground_truth.append(output_list)\n",
    "\n",
    "# print(test_data)\n",
    "# print(ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8b5081e0-e092-4d9d-8a06-6ef14a3837bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_ner(sentence):\n",
    "    tok_sentence = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_fine_tuned(**tok_sentence).logits.argmax(-1)\n",
    "        predicted_tokens_classes = [\n",
    "            model_fine_tuned.config.id2label[t.item()] for t in logits[0]]\n",
    "\n",
    "        predicted_labels = []\n",
    "\n",
    "        previous_token_id = 0\n",
    "        word_ids = tok_sentence.word_ids()\n",
    "        for word_index in range(len(word_ids)):\n",
    "            if word_ids[word_index] == None:\n",
    "                previous_token_id = word_ids[word_index]\n",
    "            elif word_ids[word_index] == previous_token_id:\n",
    "                previous_token_id = word_ids[word_index]\n",
    "            else:\n",
    "                predicted_labels.append(predicted_tokens_classes[word_index])\n",
    "                previous_token_id = word_ids[word_index]\n",
    "\n",
    "        ner_output = []\n",
    "        l = min (len(sentence.split(' ')) , len(predicted_labels) )\n",
    "        for index in range(l):\n",
    "            ner_output.append(\n",
    "                (sentence.split(' ')[index], predicted_labels[index]))\n",
    "        return ner_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8dbc698b-a114-493a-af60-58721f8cd696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str = \"दरअसल , जनवरी से चीन और नेपाल के सीमावर्ती क्षेत्रों को संचार सुविधा के लिए अब बैलून नेटवर्क सिस्टम की शुरूआत की जा रही है, जिसके साथ ही उत्तराखंड बैलून से नेटवर्क सुविधा देने वाला पहला राज्य बनेगा।  \"\n",
    "labeled_output = []\n",
    "output=[]\n",
    "for i in test_data:\n",
    "    l = get_ner(i)\n",
    "    labeled_output.append(l)\n",
    "    temp=[]\n",
    "    for j in l:\n",
    "        temp.append(j[1])\n",
    "    output.append(temp)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8453e64-e8c2-4989-8313-0cda7fcee06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(labeled_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "922f70d0-ac58-4479-8357-f4cbf6261dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dee9b81b-557d-48d6-a5ed-45866cbc3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('tags_output_BERT_test.txt', 'w') as file:\n",
    "    for tags in output:\n",
    "        tags_string = ' '.join(tags)\n",
    "        file.write(tags_string + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1dae3f4f-0e77-4fc7-9968-e0dae0111adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tags_output_BERT_ground_truth.txt', 'w') as file:\n",
    "    for tags in ground_truth:\n",
    "        tags_string = ' '.join(tags)\n",
    "        file.write(tags_string + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa51e9be-42db-4cce-b186-e36bbae9ebaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ad295-316f-4f52-ac6a-3e293b98b87a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4290140e-2ab1-4b9c-8aac-0d99a00b61c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccf5b1b-95c8-4219-81bf-d5352b480c42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a780c-a098-43a0-b724-49df23cffa7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f206649-1d4b-4ca1-ab38-d1c72ad41808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4228dba-9047-49b4-95e4-0b9d942d05de",
   "metadata": {},
   "source": [
    "# Indic-NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "165dd2a7-7670-4b8e-932c-5871e8e9008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoConfig, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForTokenClassification, EarlyStoppingCallback, IntervalStrategy\n",
    "import numpy as np\n",
    "\n",
    "tokenizer_ner = AutoTokenizer.from_pretrained(\"tokenizer_indicNER/kaggle/working/tokenizer_indicNER\")\n",
    "model_fine_tuned_ner = AutoModelForTokenClassification.from_pretrained(\"my_indicNER/kaggle/working/my_indicNER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "9b17260e-880f-4622-b99e-b02e69e9aaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_ner(sentence):\n",
    "    tok_sentence = tokenizer_ner(sentence, return_tensors='pt')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model_fine_tuned_ner(**tok_sentence).logits.argmax(-1)\n",
    "        predicted_tokens_classes = [\n",
    "            model_fine_tuned_ner.config.id2label[t.item()] for t in logits[0]]\n",
    "\n",
    "        predicted_labels = []\n",
    "\n",
    "        previous_token_id = 0\n",
    "        word_ids = tok_sentence.word_ids()\n",
    "        for word_index in range(len(word_ids)):\n",
    "            if word_ids[word_index] == None:\n",
    "                previous_token_id = word_ids[word_index]\n",
    "            elif word_ids[word_index] == previous_token_id:\n",
    "                previous_token_id = word_ids[word_index]\n",
    "            else:\n",
    "                predicted_labels.append(predicted_tokens_classes[word_index])\n",
    "                previous_token_id = word_ids[word_index]\n",
    "\n",
    "        ner_output = []\n",
    "        l = min (len(sentence.split(' ')) , len(predicted_labels) )\n",
    "        for index in range(l):\n",
    "            ner_output.append(\n",
    "                (sentence.split(' ')[index], predicted_labels[index]))\n",
    "        return ner_output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "7026fdf9-c196-463d-bff6-d9fc9e220884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str = \"दरअसल , जनवरी से चीन और नेपाल के सीमावर्ती क्षेत्रों को संचार सुविधा के लिए अब बैलून नेटवर्क सिस्टम की शुरूआत की जा रही है, जिसके साथ ही उत्तराखंड बैलून से नेटवर्क सुविधा देने वाला पहला राज्य बनेगा।  \"\n",
    "labeled_output_ner = []\n",
    "output_ner=[]\n",
    "for i in test_data:\n",
    "    l = get_ner(i)\n",
    "    labeled_output_ner.append(l)\n",
    "    temp=[]\n",
    "    for j in l:\n",
    "        temp.append(j[1])\n",
    "    output_ner.append(temp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "e63dd5ad-f752-4814-9926-4257807438cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('tags_output_NER_test.txt', 'w') as file:\n",
    "    for tags in output_ner:\n",
    "        tags_string = ' '.join(tags)\n",
    "        file.write(tags_string + '\\n' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eab04f-ffe9-4bff-b3e9-f8b1ca93eaca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
