{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6a3f3e-aa78-466b-a709-8315c90c2e4a",
   "metadata": {},
   "source": [
    "# indicBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5af5df7e-152f-444e-9b3e-c7baaea5b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "with open('tags_output_BERT_test.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "     bert_tags = file.readlines()\n",
    "\n",
    "with open('tags_output_BERT_ground_truth.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "     ground_truth = file.readlines()\n",
    "\n",
    "# print(len(bert_tags))\n",
    "# print(len(ground_truth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "555fdc4e-8313-4539-b176-76b6b5ef7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "b=[]\n",
    "for i in bert_tags:\n",
    "    temp = i.strip().split()\n",
    "    b.append(temp)\n",
    "\n",
    "g = []\n",
    "labels_list=[]\n",
    "\n",
    "for i in range(len(ground_truth)):\n",
    "    labels_list = ground_truth[i].strip().split()\n",
    "    g.append(labels_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13af23c7-69e8-45eb-9334-af624bf50573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(g)\n",
    "# print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35ce167f-6415-4515-83dd-37076ca59c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(b)\n",
    "# print(g)\n",
    "\n",
    "for i in range(867):\n",
    "    x = len(b[i])\n",
    "    y = len(g[i])\n",
    "    while(x<y):\n",
    "        b[i].append('NA')\n",
    "        x = len(b[i])\n",
    "        y = len(g[i])\n",
    "        \n",
    "    while(x>y):\n",
    "        b[i].pop()\n",
    "        x = len(b[i])\n",
    "        y = len(g[i])\n",
    "        \n",
    "        \n",
    "        \n",
    "# for i in range(25):\n",
    "#     print(i,len(b[i]),len(g[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8593c091-13c4-4ce9-b41c-4d0a3af8e307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-PER       0.85      0.89      0.87       788\n",
      "       B-ORG       0.79      0.81      0.80       521\n",
      "       B-LOC       0.82      0.81      0.82       613\n",
      "       I-PER       0.91      0.91      0.91       747\n",
      "       I-ORG       0.81      0.77      0.79       512\n",
      "           O       0.98      0.98      0.98     16513\n",
      "       I-LOC       0.80      0.59      0.68       199\n",
      "\n",
      "    accuracy                           0.95     19893\n",
      "   macro avg       0.85      0.82      0.83     19893\n",
      "weighted avg       0.95      0.95      0.95     19893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ground_truth_flat_bert = [label for sublist in g for label in sublist]\n",
    "predicted_labels_flat_bert = [label for sublist in b for label in sublist]\n",
    "\n",
    "# Print classification report\n",
    "# print(classification_report(ground_truth_flat_bert, predicted_labels_flat_bert))\n",
    "\n",
    "print(classification_report(ground_truth_flat_bert, predicted_labels_flat_bert, labels=[label for label in set(ground_truth_flat_bert) if label != 'NA']))\n",
    "\n",
    "# print(len(ground_truth_flat_bert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "94eccfcd-fc5d-4664-9aa4-561da438ebbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.6490502408696407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rubyp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_scores_bert = f1_score(ground_truth_flat_bert, predicted_labels_flat_bert, labels=['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'], average=None)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1_bert = sum(f1_scores_bert) / len(f1_scores_bert)\n",
    "\n",
    "print(\"Macro F1 Score:\", macro_f1_bert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6293c8-656f-4cd4-8353-2884531a4c47",
   "metadata": {},
   "source": [
    "# indicNER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "028c3304-a257-4980-aa97-dcef9a54ca5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "with open('tags_output_NER_test.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "     ner_tags = file.readlines()\n",
    "\n",
    "# print(len(bert_tags))\n",
    "# print(len(ground_truth))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d71d41e5-7a77-41ee-994d-b0b80942628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth\n",
    "n=[]\n",
    "for i in ner_tags:\n",
    "    temp = i.strip().split()\n",
    "    n.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "656cc9ba-85fd-4c9f-8983-7dc7dc6a4564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(b)\n",
    "# print(g)\n",
    "\n",
    "for i in range(867):\n",
    "    x = len(n[i])\n",
    "    y = len(g[i])\n",
    "    while(x<y):\n",
    "        b[i].append('NA')\n",
    "        x = len(n[i])\n",
    "        y = len(g[i])\n",
    "        \n",
    "    while(x>y):\n",
    "        b[i].pop()\n",
    "        x = len(n[i])\n",
    "        y = len(g[i])\n",
    "        \n",
    "        \n",
    "        \n",
    "# for i in range(25):\n",
    "#     print(i,len(b[i]),len(g[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "31f22f76-3bae-4114-85e5-aae4acd64270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-PER       0.88      0.92      0.90       788\n",
      "       B-ORG       0.74      0.83      0.79       521\n",
      "       B-LOC       0.85      0.87      0.86       613\n",
      "       I-PER       0.90      0.94      0.92       747\n",
      "       I-ORG       0.68      0.78      0.73       512\n",
      "           O       0.98      0.97      0.98     16513\n",
      "       I-LOC       0.83      0.66      0.74       199\n",
      "\n",
      "    accuracy                           0.95     19893\n",
      "   macro avg       0.84      0.85      0.84     19893\n",
      "weighted avg       0.96      0.95      0.95     19893\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ground_truth_flat_ner = [label for sublist in g for label in sublist]\n",
    "predicted_labels_flat_ner = [label for sublist in n for label in sublist]\n",
    "\n",
    "# Print classification report\n",
    "# print(classification_report(ground_truth_flat_bert, predicted_labels_flat_bert))\n",
    "\n",
    "print(classification_report(ground_truth_flat_ner, predicted_labels_flat_ner, labels=[label for label in set(ground_truth_flat_ner) if label != 'NA']))\n",
    "\n",
    "# print(len(ground_truth_flat_bert))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f66c6538-2995-498f-bbc5-7d680a353d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 Score: 0.6563827168901009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rubyp\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_scores_ner = f1_score(ground_truth_flat_ner, predicted_labels_flat_ner, labels=['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'], average=None)\n",
    "\n",
    "# Calculate macro F1 score\n",
    "macro_f1_ner = sum(f1_scores_ner) / len(f1_scores_ner)\n",
    "\n",
    "print(\"Macro F1 Score:\", macro_f1_ner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974af5ec-5071-4571-8c1d-14afbabf69b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
